{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "seed = 1234\n",
    "\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "# fix random seed for reproducibility\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.random.set_random_seed(seed)\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import text_utilities as tu\n",
    "import modeling_utils as mu\n",
    "import image_utilities as iu\n",
    "import ocr\n",
    "import doc_classifier_model as dcm\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.optimizers import Adagrad\n",
    "from keras import backend as K\n",
    "import json\n",
    "import keras\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "sess_config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1,\n",
    "allow_soft_placement=True, device_count = {'CPU': 1})\n",
    "sess = tf.Session(graph=tf.get_default_graph(),config=sess_config)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 29366\n",
      "----------------------------------------\n",
      "Train X Size: (1316, 2)\n",
      "Train Y Size: (1316,)\n",
      "----------------------------------------\n",
      "Test X size: (565, 2)\n",
      "Test Y Size: (565,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv('data/data_df_new.csv')\n",
    "# data_df = pd.read_csv('/home/sureclaim/Documents/Claims/data_df.csv')\n",
    "data_df['y_oneh'] = mu.onehot_encode(data_df.y)\n",
    "data_df = data_df.dropna()\n",
    "# Clean the text column; keep only alphabets\n",
    "data_df['x_txt_cleaned'] = data_df.x_txt.apply(tu.clean_string)\n",
    "max_len = 70\n",
    "vocab = tu.make_vocab(data_df.x_txt_cleaned)\n",
    "w2i = tu.make_w2i(vocab)\n",
    "\n",
    "# Shuffle dataframe\n",
    "data_df = data_df.sample(frac=1)\n",
    "\n",
    "# Get word level splits\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_df[['path', 'x_txt_cleaned']], data_df.y_oneh, test_size=0.3, random_state=201)\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Train X Size:\", x_train.shape)\n",
    "print(\"Train Y Size:\", y_train.shape)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Test X size:\", x_test.shape)\n",
    "print(\"Test Y Size:\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X text train tensor shape: (1316, 70)\n",
      "X images train tensor shape: (1316, 131072)\n",
      "y train shape: (1316, 10)\n",
      "----------------------------------------\n",
      "X text test tensor shape: (565, 70)\n",
      "X images test tensor shape: (565, 131072)\n",
      "y test shape: (565, 10)\n"
     ]
    }
   ],
   "source": [
    "# make tensors\n",
    "x_txt_train = tu.make_tensor_np(x_train.x_txt_cleaned, w2i, max_len)\n",
    "#x_img_train = np.stack(x_train.path.apply(generate_image_features, args=[resnet50]))\n",
    "#np.save('data/img_features_train.npy', x_img_train)\n",
    "x_img_train = np.load('data/img_features_train.npy')\n",
    "y_train = np.stack(y_train.to_numpy())\n",
    "\n",
    "x_txt_test = tu.make_tensor_np(x_test.x_txt_cleaned, w2i, max_len)\n",
    "#x_img_test = np.stack(x_test.path.apply(generate_image_features, args=[resnet50]))\n",
    "#np.save('data/img_features_test.npy', x_img_test)\n",
    "x_img_test = np.load('data/img_features_test.npy')\n",
    "y_test = np.stack(y_test.to_numpy())\n",
    "\n",
    "print(\"X text train tensor shape:\", x_txt_train.shape)\n",
    "print(\"X images train tensor shape:\", x_img_train.shape)\n",
    "print(\"y train shape:\", y_train.shape)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"X text test tensor shape:\", x_txt_test.shape)\n",
    "print(\"X images test tensor shape:\", x_img_test.shape)\n",
    "print(\"y test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Restored model, train accuracy: 89.51%\n",
      "Restored model, test accuracy: 72.74%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# Add ops to save and restore all the variables.\n",
    "#saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    json_file = open('model2.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = keras.models.model_from_json(loaded_model_json, custom_objects={'SeqSelfAttention': SeqSelfAttention})\n",
    "    loaded_model.load_weights('model2.h5')\n",
    "\n",
    "    opt = Adagrad(lr = 1e-3)\n",
    "        #sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    #saver = tf.train.Saver()\n",
    "    # Restore variables from disk.\n",
    "    #saver.restore(sess, \"models/sess.ckpt\")\n",
    "    #print(\"Session restored.\")\n",
    "    \n",
    "    # Re-evaluate the model\n",
    "    with graph.as_default():\n",
    "        loss,acc = loaded_model.evaluate([x_img_train, x_txt_train], y_train, verbose=2)\n",
    "    print(\"Restored model, train accuracy: {:5.2f}%\".format(100*acc))\n",
    "    with graph.as_default():\n",
    "        loss,acc = loaded_model.evaluate([x_img_test, x_txt_test], y_test, verbose=2)\n",
    "    print(\"Restored model, test accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored model, train accuracy: 89.74%\n",
      "Restored model, test accuracy: 72.57%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    graph = tf.get_default_graph()\n",
    "    with open('model_config', 'r') as configfile:\n",
    "        config = json.load(configfile)\n",
    "    \n",
    "    loaded_model = dcm.build_doc_classifier(config['input_text_shape'], config['input_img_shape'], config['n_classes'], config['vocab_size'])\n",
    "    loaded_model.load_weights('model2.h5')\n",
    "    opt = Adagrad(lr = 1e-3)\n",
    "    loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "    # Re-evaluate the model\n",
    "    with graph.as_default():\n",
    "        loss,acc = loaded_model.evaluate([x_img_train, x_txt_train], y_train, verbose=2)\n",
    "    print(\"Restored model, train accuracy: {:5.2f}%\".format(100*acc))\n",
    "    with graph.as_default():\n",
    "        loss,acc = loaded_model.evaluate([x_img_test, x_txt_test], y_test, verbose=2)\n",
    "    print(\"Restored model, test accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7292035398230089"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with open('model_config', 'r') as configfile:\n",
    "        config = json.load(configfile)\n",
    "    \n",
    "    loaded_model = dcm.build_doc_classifier(config['input_text_shape'], config['input_img_shape'], config['n_classes'], config['vocab_size'])\n",
    "    loaded_model.load_weights('model.h5')\n",
    "    opt = Adagrad(lr = 1e-3)\n",
    "    loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "    with graph.as_default():\n",
    "        scores = loaded_model.predict([x_img_test, x_txt_test], verbose=2)\n",
    "y_hat = np.array([pred.argmax() for pred in scores])\n",
    "y_act = np.array([act.argmax() for act in y_test])\n",
    "\n",
    "correct = 0\n",
    "total=0\n",
    "for i in range(len(y_hat)):\n",
    "    if y_hat[i] == y_act[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "seed = 1234\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "# fix random seed for reproducibility\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.random.set_random_seed(seed)\n",
    "import keras\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import text_utilities as tu\n",
    "import modeling_utils as mu\n",
    "import image_utilities as iu\n",
    "import ocr\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import doc_classifier_model as dcm\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.optimizers import Adagrad\n",
    "from keras import backend as K\n",
    "import json\n",
    "import time\n",
    "import resnet50_feature_extractor as rfe\n",
    "import operator\n",
    "from PIL import Image\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from keras.applications import ResNet50\n",
    "\n",
    "sess_config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1,\n",
    "allow_soft_placement=True, device_count = {'CPU': 1})\n",
    "sess = tf.Session(graph=tf.get_default_graph(),config=sess_config)\n",
    "K.set_session(sess)\n",
    "\n",
    "with open('model_config', 'r') as configfile:\n",
    "    config = json.load(configfile)\n",
    "\n",
    "'''tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = keras.models.model_from_json(loaded_model_json, custom_objects={'SeqSelfAttention': SeqSelfAttention})\n",
    "    loaded_model.load_weights('model.h5')\n",
    "    \n",
    "    loaded_model = dcm.build_doc_classifier(config['input_text_shape'], config['input_img_shape'], config['n_classes'], config['vocab_size'])\n",
    "    loaded_model.load_weights(\"models/doc_classifier.h5\")\n",
    "\n",
    "    opt = Adagrad(lr = 1e-3)\n",
    "    loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    #saver = tf.train.Saver()\n",
    "    # Restore variables from disk.\n",
    "    #saver.restore(sess, \"models/sess.ckpt\")\n",
    "    #print(\"Session restored.\")'''\n",
    "\n",
    "# Function to decode image to jpg from base64\n",
    "def decode_image(image_path):\n",
    "    with open(image_path, 'r') as data_file:\n",
    "        data = data_file.read()\n",
    "    data = json.loads(data)\n",
    "\n",
    "    return Image.open(BytesIO(base64.b64decode(data['base64Data'][0])))\n",
    "\n",
    "def predict(image_path):\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    image = decode_image(image_path)\n",
    "\n",
    "    # Create text features\n",
    "    x_txt = tu.clean_string(ocr.get_text_from_image(image))\n",
    "    x_txt = tu.make_tensor_np(x_txt, config['w2i'], config['input_text_shape'])\n",
    "    \n",
    "    # Create image features\n",
    "    x_img = np.squeeze(rfe.generate_image_features3(image))\n",
    "    \n",
    "    # Make model prediction\n",
    "    with tf.Session() as sess:\n",
    "        loaded_model = dcm.build_doc_classifier(config['input_text_shape'], config['input_img_shape'], config['n_classes'], config['vocab_size'])\n",
    "        loaded_model.load_weights(\"model2.h5\")\n",
    "  \n",
    "        opt = Adagrad(lr = 1e-3)\n",
    "        loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "        with graph.as_default():\n",
    "            scores = loaded_model.predict([x_img.reshape(-1, config['input_img_shape']), x_txt.reshape(-1, config['input_text_shape'])]).tolist()\n",
    "    classes = ['Aadhar Card', 'Diagnostic Bill', 'Discharge Summary', 'Insurance Card', 'Internal Case Papers', 'Pan Card', 'Phramacy Bill', 'Policy Copy',\t'Prescriptions' , 'Receipts']\n",
    "    \n",
    "    # Prepare response and return\n",
    "    response = dict(zip(classes, scores[0]))\n",
    "    print('Prediction:', max(response.items(), key=operator.itemgetter(1))[0])\n",
    "    \n",
    "    end = time.time()\n",
    "    dur = end-start\n",
    "    \n",
    "    if dur<60:\n",
    "        print(\"Execution Time:\",dur,\"seconds\")\n",
    "    elif dur>60 and dur<3600:\n",
    "        dur=dur/60\n",
    "        print(\"Execution Time:\",dur,\"minutes\")\n",
    "    else:\n",
    "        dur=dur/(60*60)\n",
    "        print(\"Execution Time:\",dur,\"hours\")\n",
    "        \n",
    "    return json.dumps(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sureclaim/.local/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Insurance Card\n",
      "Execution Time: 18.91075325012207 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"Aadhar Card\": 0.011417805217206478, \"Diagnostic Bill\": 0.0021242836955934763, \"Discharge Summary\": 0.16710777580738068, \"Insurance Card\": 0.6064165234565735, \"Internal Case Papers\": 0.010215921327471733, \"Pan Card\": 0.02433878183364868, \"Phramacy Bill\": 0.047197531908750534, \"Policy Copy\": 0.00018836144590750337, \"Prescriptions\": 0.12591440975666046, \"Receipts\": 0.005078513640910387}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('/home/sureclaim/Documents/Claims/sampleJsonData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess_config = tf.ConfigProto()\n",
    "\n",
    "class LoadModel:\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        #with K.get_session().graph.as_default():\n",
    "        self.sess  = tf.Session(config=sess_config)\n",
    "        K.set_session(self.sess) \n",
    "        print('Loading ResNet50...')\n",
    "        self.resnet50 = ResNet50(weights='imagenet', pooling=max, include_top=False)\n",
    "        print('ResNet50 Loaded...')\n",
    "        print('Loading Doc Classifier...')\n",
    "        self.model = dcm.build_doc_classifier(config['input_text_shape'], config['input_img_shape'], config['n_classes'], config['vocab_size'])\n",
    "        self.model.load_weights(model_path)\n",
    "        print('Doc Classifier Loaded...')\n",
    "        self.opt = Adagrad(lr = 1e-3)\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=self.opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "        self.graph = tf.get_default_graph()\n",
    "\n",
    "    # Function to decode image to jpg from base64\n",
    "    def decode_image(self, image_path):\n",
    "        with open(image_path, 'r') as data_file:\n",
    "            self.data = data_file.read()\n",
    "        self.data = json.loads(self.data)\n",
    "        return Image.open(BytesIO(base64.b64decode(self.data['base64Data'][0])))\n",
    "    \n",
    "    \n",
    "    # Function to generate resnet features\n",
    "    def generate_image_features(self, image, reshape=(225, 225)):\n",
    "    \n",
    "        # GENERATING FEATURES\n",
    "        self.image = image.resize(reshape)\n",
    "        self.image.load()\n",
    "        self.x = np.asarray(self.image, dtype=\"int32\" )\n",
    "        self.x = np.expand_dims(self.x, axis=0) \n",
    "        with self.graph.as_default():\n",
    "            self.img_features = self.resnet50.predict(self.x, verbose=2) \n",
    "        self.img_features = self.img_features.squeeze() \n",
    "        self.img_features = self.img_features.flatten()\n",
    "        return self.img_features\n",
    "\n",
    "    def predict(self, image_path):\n",
    "    \n",
    "        self.start = time.time()\n",
    "\n",
    "        self.img = self.decode_image(image_path)\n",
    "\n",
    "        # Create text features\n",
    "        self.x_txt = tu.clean_string(ocr.get_text_from_image(self.img))\n",
    "        self.x_txt = tu.make_tensor_np(self.x_txt, config['w2i'], config['input_text_shape'])\n",
    "    \n",
    "        # Create image features\n",
    "        self.x_img = np.squeeze(self.generate_image_features(self.img))\n",
    "        \n",
    "        # Make model prediction\n",
    "        #with tf.Session() as sess:\n",
    "\n",
    "            #graph = tf.get_default_graph()\n",
    "        with self.graph.as_default():\n",
    "            self.scores = self.model.predict([self.x_img.reshape(-1, config['input_img_shape']), self.x_txt.reshape(-1, config['input_text_shape'])]).tolist()\n",
    "            #_, self.acc = self.model.evaluate([x_img_train, x_txt_train], y_train, verbose=2)\n",
    "            #print(\"Restored model, train accuracy: {:5.2f}%\".format(100*self.acc))\n",
    "            #_, self.acc = self.model.evaluate([x_img_test, x_txt_test], y_test, verbose=2)\n",
    "            #print(\"Restored model, test accuracy: {:5.2f}%\".format(100*self.acc))\n",
    "        \n",
    "        self.classes = ['Aadhar Card', 'Diagnostic Bill', 'Discharge Summary', 'Insurance Card', 'Internal Case Papers', 'Pan Card', 'Phramacy Bill', 'Policy Copy',\t'Prescriptions' , 'Receipts']\n",
    "    \n",
    "        # Prepare response and return\n",
    "        self.response = dict(zip(self.classes, self.scores[0]))\n",
    "        print('Prediction:', max(self.response.items(), key=operator.itemgetter(1))[0])\n",
    "    \n",
    "        self.end = time.time()\n",
    "        self.dur = self.end - self.start\n",
    "    \n",
    "        if self.dur<60:\n",
    "            print(\"Execution Time:\", self.dur,\"seconds\")\n",
    "        elif self.dur>60 and self.dur<3600:\n",
    "            self.dur=dur/60\n",
    "            print(\"Execution Time:\", self.dur,\"minutes\")\n",
    "        else:\n",
    "            self.dur = self.dur/(60*60)\n",
    "            print(\"Execution Time:\", self.dur,\"hours\")\n",
    "        \n",
    "        return json.dumps(self.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ResNet50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sureclaim/.local/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 Loaded...\n",
      "Loading Doc Classifier...\n",
      "Doc Classifier Loaded...\n"
     ]
    }
   ],
   "source": [
    "loaded_model = LoadModel('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored model, train accuracy: 89.51%\n",
      "Restored model, test accuracy: 72.74%\n",
      "Prediction: Insurance Card\n",
      "Execution Time: 13.2495596408844 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"Aadhar Card\": 0.011417805217206478, \"Diagnostic Bill\": 0.0021242836955934763, \"Discharge Summary\": 0.16710777580738068, \"Insurance Card\": 0.6064165234565735, \"Internal Case Papers\": 0.010215921327471733, \"Pan Card\": 0.02433878183364868, \"Phramacy Bill\": 0.047197531908750534, \"Policy Copy\": 0.00018836144590750337, \"Prescriptions\": 0.12591440975666046, \"Receipts\": 0.005078513640910387}'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict('/home/sureclaim/Documents/Claims/sampleJsonData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
